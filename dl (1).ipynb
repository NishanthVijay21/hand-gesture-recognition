{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32a15b9c-67e7-4caf-957a-23bbff3bfe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nandu\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 720ms/step - accuracy: 0.6680 - loss: 5.6892 - val_accuracy: 0.8682 - val_loss: 0.3705\n",
      "Epoch 2/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 676ms/step - accuracy: 0.8945 - loss: 0.2854 - val_accuracy: 0.9807 - val_loss: 0.0536\n",
      "Epoch 3/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 678ms/step - accuracy: 0.9284 - loss: 0.1849 - val_accuracy: 0.9854 - val_loss: 0.0354\n",
      "Epoch 4/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 671ms/step - accuracy: 0.9519 - loss: 0.1141 - val_accuracy: 0.9953 - val_loss: 0.0175\n",
      "Epoch 5/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 667ms/step - accuracy: 0.9555 - loss: 0.1030 - val_accuracy: 0.9984 - val_loss: 0.0091\n",
      "Epoch 6/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 688ms/step - accuracy: 0.9613 - loss: 0.0864 - val_accuracy: 0.9953 - val_loss: 0.0131\n",
      "Epoch 7/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 672ms/step - accuracy: 0.9688 - loss: 0.0669 - val_accuracy: 0.9896 - val_loss: 0.0284\n",
      "Epoch 8/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 669ms/step - accuracy: 0.9694 - loss: 0.0753 - val_accuracy: 0.9943 - val_loss: 0.0127\n",
      "Epoch 9/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 679ms/step - accuracy: 0.9684 - loss: 0.0616 - val_accuracy: 0.9937 - val_loss: 0.0200\n",
      "Epoch 10/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 702ms/step - accuracy: 0.9732 - loss: 0.0641 - val_accuracy: 0.9995 - val_loss: 0.0087\n",
      "Epoch 11/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 686ms/step - accuracy: 0.9725 - loss: 0.0557 - val_accuracy: 0.9984 - val_loss: 0.0088\n",
      "Epoch 12/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 670ms/step - accuracy: 0.9736 - loss: 0.0545 - val_accuracy: 0.9974 - val_loss: 0.0086\n",
      "Epoch 13/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 667ms/step - accuracy: 0.9795 - loss: 0.0563 - val_accuracy: 0.9984 - val_loss: 0.0133\n",
      "Epoch 14/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 667ms/step - accuracy: 0.9730 - loss: 0.0601 - val_accuracy: 0.9990 - val_loss: 0.0128\n",
      "Epoch 15/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 668ms/step - accuracy: 0.9817 - loss: 0.0454 - val_accuracy: 0.9995 - val_loss: 0.0092\n",
      "Epoch 16/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 667ms/step - accuracy: 0.9843 - loss: 0.0341 - val_accuracy: 0.9995 - val_loss: 0.0101\n",
      "Epoch 17/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 674ms/step - accuracy: 0.9815 - loss: 0.0396 - val_accuracy: 0.9969 - val_loss: 0.0320\n",
      "Epoch 18/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 691ms/step - accuracy: 0.9801 - loss: 0.0408 - val_accuracy: 0.9995 - val_loss: 0.0184\n",
      "Epoch 19/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 674ms/step - accuracy: 0.9802 - loss: 0.0417 - val_accuracy: 0.9984 - val_loss: 0.0146\n",
      "Epoch 20/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 675ms/step - accuracy: 0.9796 - loss: 0.0397 - val_accuracy: 0.9984 - val_loss: 0.0157\n",
      "Epoch 21/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 679ms/step - accuracy: 0.9773 - loss: 0.0468 - val_accuracy: 0.9984 - val_loss: 0.0153\n",
      "Epoch 22/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 666ms/step - accuracy: 0.9806 - loss: 0.0418 - val_accuracy: 0.9958 - val_loss: 0.0162\n",
      "Epoch 23/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 672ms/step - accuracy: 0.9830 - loss: 0.0394 - val_accuracy: 0.9984 - val_loss: 0.0169\n",
      "Epoch 24/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 669ms/step - accuracy: 0.9831 - loss: 0.0358 - val_accuracy: 0.9995 - val_loss: 0.0071\n",
      "Epoch 25/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 667ms/step - accuracy: 0.9861 - loss: 0.0309 - val_accuracy: 0.9995 - val_loss: 0.0133\n",
      "Epoch 26/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 667ms/step - accuracy: 0.9854 - loss: 0.0289 - val_accuracy: 0.9979 - val_loss: 0.0080\n",
      "Epoch 27/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 676ms/step - accuracy: 0.9872 - loss: 0.0284 - val_accuracy: 0.9990 - val_loss: 0.0135\n",
      "Epoch 28/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 668ms/step - accuracy: 0.9873 - loss: 0.0406 - val_accuracy: 0.9995 - val_loss: 0.0189\n",
      "Epoch 29/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 671ms/step - accuracy: 0.9823 - loss: 0.0414 - val_accuracy: 0.9995 - val_loss: 0.0155\n",
      "Epoch 30/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 670ms/step - accuracy: 0.9800 - loss: 0.0742 - val_accuracy: 0.9990 - val_loss: 0.0211\n",
      "Epoch 31/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 667ms/step - accuracy: 0.9813 - loss: 0.0557 - val_accuracy: 0.9995 - val_loss: 0.0221\n",
      "Epoch 32/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 666ms/step - accuracy: 0.9837 - loss: 0.0385 - val_accuracy: 0.9995 - val_loss: 0.0214\n",
      "Epoch 33/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 694ms/step - accuracy: 0.9854 - loss: 0.0320 - val_accuracy: 0.9990 - val_loss: 0.0239\n",
      "Epoch 34/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 666ms/step - accuracy: 0.9890 - loss: 0.0300 - val_accuracy: 0.9995 - val_loss: 0.0249\n",
      "Epoch 35/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 669ms/step - accuracy: 0.9918 - loss: 0.0207 - val_accuracy: 0.9995 - val_loss: 0.0148\n",
      "Epoch 36/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 670ms/step - accuracy: 0.9909 - loss: 0.0220 - val_accuracy: 0.9990 - val_loss: 0.0046\n",
      "Epoch 37/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 668ms/step - accuracy: 0.9946 - loss: 0.0163 - val_accuracy: 0.9995 - val_loss: 0.0111\n",
      "Epoch 38/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 676ms/step - accuracy: 0.9932 - loss: 0.0151 - val_accuracy: 0.9937 - val_loss: 0.0426\n",
      "Epoch 39/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 671ms/step - accuracy: 0.9923 - loss: 0.0180 - val_accuracy: 0.9984 - val_loss: 0.0373\n",
      "Epoch 40/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 669ms/step - accuracy: 0.9936 - loss: 0.0159 - val_accuracy: 0.9995 - val_loss: 0.0161\n",
      "Epoch 41/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 665ms/step - accuracy: 0.9925 - loss: 0.0161 - val_accuracy: 0.9990 - val_loss: 0.0120\n",
      "Epoch 42/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 670ms/step - accuracy: 0.9927 - loss: 0.0137 - val_accuracy: 0.9995 - val_loss: 0.0171\n",
      "Epoch 43/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 676ms/step - accuracy: 0.9931 - loss: 0.0173 - val_accuracy: 0.9995 - val_loss: 0.0157\n",
      "Epoch 44/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 682ms/step - accuracy: 0.9936 - loss: 0.0188 - val_accuracy: 0.9974 - val_loss: 0.0255\n",
      "Epoch 45/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 668ms/step - accuracy: 0.9936 - loss: 0.0208 - val_accuracy: 0.9995 - val_loss: 0.0226\n",
      "Epoch 46/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 668ms/step - accuracy: 0.9915 - loss: 0.0232 - val_accuracy: 0.9995 - val_loss: 0.0121\n",
      "Epoch 47/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 670ms/step - accuracy: 0.9946 - loss: 0.0154 - val_accuracy: 0.9995 - val_loss: 0.0120\n",
      "Epoch 48/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 668ms/step - accuracy: 0.9922 - loss: 0.0242 - val_accuracy: 0.9995 - val_loss: 0.0048\n",
      "Epoch 49/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 668ms/step - accuracy: 0.9945 - loss: 0.0138 - val_accuracy: 0.9995 - val_loss: 0.0093\n",
      "Epoch 50/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 668ms/step - accuracy: 0.9958 - loss: 0.0104 - val_accuracy: 0.9995 - val_loss: 5.9187e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Set data directory path\n",
    "DATASET_PATH = \"C:/Users/nandu/Downloads/archive/data\"\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset():\n",
    "    loaded_images = []\n",
    "    list_of_gestures = ['blank', 'ok', 'thumbsup', 'thumbsdown', 'fist', 'five']\n",
    "    \n",
    "    for gesture in list_of_gestures:\n",
    "        gesture_path = os.path.join(f'C:/Users/nandu/Downloads/archive/data/{gesture}', '*')\n",
    "        images = glob.glob(gesture_path)\n",
    "        for img_path in images[:1600]:  # Use a limit of 1600 images per gesture\n",
    "            image = cv2.imread(img_path)\n",
    "            gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            gray_image = cv2.resize(gray_image, (100, 120))\n",
    "            loaded_images.append(gray_image)\n",
    "\n",
    "    outputVectors = []\n",
    "    for i in range(6):  # For each of the 6 gestures\n",
    "        outputVectors.extend([[1 if j == i else 0 for j in range(6)]] * 1600)\n",
    "    \n",
    "    X = np.asarray(loaded_images)\n",
    "    y = np.asarray(outputVectors)\n",
    "    \n",
    "    X = X.reshape(X.shape[0], 100, 120, 1)\n",
    "    return X, y\n",
    "\n",
    "# CNN model creation\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(100, 120, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(6, activation='softmax'))  # 6 gestures\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train model\n",
    "def train_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train, batch_size=128, epochs=50, verbose=1, validation_data=(X_test, y_test))\n",
    "    model.save('C:/Users/nandu/Downloads/archive/data/hand_gesture_recognition.h5')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess the dataset\n",
    "    X, y = load_dataset()\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = create_model()\n",
    "    train_model(model, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38efb9be-69d1-41a0-8a8f-7f153e6747e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "# Initialize the background for background subtraction\n",
    "bg = None\n",
    "\n",
    "# Function to segment the hand region\n",
    "def segment(image, threshold=25):\n",
    "    global bg\n",
    "    # find the absolute difference between background and current frame\n",
    "    diff = cv2.absdiff(bg.astype(\"uint8\"), image)\n",
    "    # threshold the diff image so that we get the foreground\n",
    "    thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)[1]\n",
    "    # get the contours in the thresholded image\n",
    "    (cnts, _) = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # return None, if no contours detected\n",
    "    if len(cnts) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # based on contour area, get the maximum contour which is the hand\n",
    "        segmented = max(cnts, key=cv2.contourArea)\n",
    "        return (thresholded, segmented)\n",
    "\n",
    "# Function to capture background for background subtraction\n",
    "def run_avg(image, accumWeight):\n",
    "    global bg\n",
    "    if bg is None:\n",
    "        bg = image.copy().astype(\"float\")\n",
    "        return\n",
    "    cv2.accumulateWeighted(image, bg, accumWeight)\n",
    "\n",
    "# Load Model Weights\n",
    "def _load_weights():\n",
    "    try:\n",
    "        model = load_model('C:/Users/nandu/Downloads/archive/data/hand_gesture_recognition.h5')\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Predict the class of the gesture from the live feed\n",
    "def getPredictedClass(model):\n",
    "    image = cv2.imread('Temp.png')\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_image = cv2.resize(gray_image, (100, 120))\n",
    "    gray_image = gray_image.reshape(1, 100, 120, 1)\n",
    "    prediction = model.predict_on_batch(gray_image)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    \n",
    "    gestures = [\"Blank\", \"OK\", \"Thumbs Up\", \"Thumbs Down\", \"Fist\", \"Five\"]\n",
    "    return gestures[predicted_class]\n",
    "\n",
    "# Run live video feed to detect hand gesture\n",
    "def run_live_feed():\n",
    "    accumWeight = 0.5\n",
    "    camera = cv2.VideoCapture(0)  # Webcam capture\n",
    "    top, right, bottom, left = 10, 350, 225, 590\n",
    "    num_frames = 0\n",
    "    model = _load_weights()\n",
    "\n",
    "    while True:\n",
    "        grabbed, frame = camera.read()\n",
    "        frame = cv2.resize(frame, (700, 700))\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        clone = frame.copy()\n",
    "        roi = frame[top:bottom, right:left]\n",
    "        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "\n",
    "        if num_frames < 30:\n",
    "            run_avg(gray, accumWeight)\n",
    "        else:\n",
    "            hand = segment(gray)\n",
    "            if hand is not None:\n",
    "                (thresholded, segmented) = hand\n",
    "                cv2.drawContours(clone, [segmented + (right, top)], -1, (0, 0, 255))\n",
    "                cv2.imwrite('Temp.png', thresholded)\n",
    "                predictedClass = getPredictedClass(model)\n",
    "                cv2.putText(clone, predictedClass, (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                cv2.imshow(\"Thresholded\", thresholded)\n",
    "        \n",
    "        cv2.rectangle(clone, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        num_frames += 1\n",
    "        cv2.imshow(\"Video Feed\", clone)\n",
    "\n",
    "        keypress = cv2.waitKey(1) & 0xFF\n",
    "        if keypress == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# ----------------- Main Execution -----------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run live video feed for hand gesture recognition\n",
    "    run_live_feed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f659d445-3420-43c8-b0d7-7d2030bd071d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b9e777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1f5671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
